{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9429824561403509\n",
      "Sensitivity: 0.8674698795180723\n",
      "Specificity: 0.9862068965517241\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load data\n",
    "data = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "feature_list = list(data[0])\n",
    "x = data[0]\n",
    "y = data[1]\n",
    "\n",
    "# Standardize inputs (for Principal Component Analysis, which I will explain later)\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "# split train and test data, allotting 40% of the data to test data.\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.4, random_state=0)\n",
    "\n",
    "'''\n",
    "    For a really big feature set, like MNIST dataset, that has 784 features, it will\n",
    "take a really long time to fit a model for prediction using all of its features.\n",
    "    For this problem, we will perform Principal Component Analysis (or PCA) to find out what\n",
    "features will take the most importance in determining the classification of wine.\n",
    "    In PCA, we will find the features that has the largest variance ration in the\n",
    "set of features. Then, we will use that features to train our model.\n",
    "'''\n",
    "# perform PCA, keeping only 6 of the principal components (not using all 12 features)\n",
    "# since 5 components, constitute more than 80% of the variance of the data\n",
    "# you can find out how many principal components you will use in this code:\n",
    "# pca = PCA(n_components=12)\n",
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "# plt.xlabel('number of components')\n",
    "# plt.ylabel('cumulative explained variance');\n",
    "pca = PCA(n_components=5)\n",
    "principal_components = pca.fit_transform(train_x)\n",
    "\n",
    "# find what are the principal components\n",
    "n_components = 5\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_components)]\n",
    "most_important_names = [feature_list[most_important[i]] for i in range(n_components)]\n",
    "principal_component_names = {'PC{}'.format(i): most_important_names[i] for i in range(n_components)}\n",
    "\n",
    "# principal components are: mean concave points, mean fractal dimension, texture error, worst texture, concavity error\n",
    "# now let's try to train a logistic regression model.\n",
    "train_x_df = pd.DataFrame(train_x, columns=feature_list)\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(train_x_df[most_important_names], train_y)\n",
    "\n",
    "# now let's predict from the test set.\n",
    "test_x_df = pd.DataFrame(test_x, columns=feature_list)\n",
    "y_hat = logistic_model.predict(test_x_df[most_important_names])\n",
    "\n",
    "# compute confusion matrix\n",
    "confusion_matrix = confusion_matrix(test_y, y_hat)\n",
    "# Confusion matrix outputs [[TP, FN],[FP, TN]]\n",
    "# Results are:\n",
    "# TP = 72\n",
    "# TN = 143\n",
    "# FP = 2\n",
    "# FN = 11\n",
    "n_samples = sum(sum(confusion_matrix))\n",
    "accuracy = (confusion_matrix[0, 0] + confusion_matrix[1, 1]) / n_samples\n",
    "sensitivity = confusion_matrix[0, 0] / (confusion_matrix[0, 0] + confusion_matrix[0, 1])\n",
    "specificity = confusion_matrix[1, 1]/(confusion_matrix[1, 0] + confusion_matrix[1, 1])\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Sensitivity: \" + str(sensitivity))\n",
    "print(\"Specificity: \" + str(specificity))\n",
    "\n",
    "# Results:\n",
    "# Accuracy: 0.9429824561403509\n",
    "# Sensitivity: 0.8674698795180723\n",
    "# Specificity: 0.9862068965517241\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
